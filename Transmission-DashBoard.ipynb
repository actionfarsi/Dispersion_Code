{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data analysis for converting comb tooth beats vs. time, to frequency vs. time, to transmission versus frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import average, hstack, argsort, ones, sort, diff, where\n",
    "\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import savemat, loadmat\n",
    "from matplotlib import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage.measurements import label, find_objects, center_of_mass\n",
    "from scipy.ndimage.filters import sobel, gaussian_filter1d, median_filter, laplace, uniform_filter1d\n",
    "from scipy.signal import find_peaks_cwt\n",
    "\n",
    "\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline, interp1d, UnivariateSpline, splrep\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot,iplot_mpl\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rollingWindow(a, window, edge = 'copy'):\n",
    "    if edge == 'copy':\n",
    "        extended = np.zeros(a.shape[:-1] + (a.shape[-1]+window-1, ))\n",
    "        extended[..., int(window/2.-1): -int(window/2.)] = a[...]\n",
    "        extended[..., :int(window/2.+1)]= a[...,0]\n",
    "        extended[..., -int(window/2.):]= a[...,-1]\n",
    "        a = extended\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "def rollingAverage(a,win_size=10):\n",
    "    out=average(rollingWindow(a, win_size), -1)\n",
    "    #out1=np.append(np.zeros(int(window/2)),out)\n",
    "    #out2=out1[:-int(window/2)] #these two modiciations make the filter symetric\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below, we select the file we wish to process, and a sample of the Transmission data, and the demodulation channel is shown. If you see no resonances in the transmisison spectrum, or no peaks in the demodulation channel, something has gone wrong in data collection and processing is impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename='rb_1555_1570_200nm_1'\n",
    "#filename='test'\n",
    "d = loadmat(filename + '.mat', squeeze_me = True)\n",
    "scale = 1e-9\n",
    "t_range = [int(1e3),int(1e7)]\n",
    "ch_n = 2\n",
    "\n",
    "ch = [ d['ch{:}'.format(c+1)][:,t_range[0]:t_range[-1]] for c in range(ch_n) ]\n",
    "trans = d['trans'][:,t_range[0]:t_range[-1]]\n",
    "\n",
    "# Start from 0\n",
    "trans[0] = (trans[0]-trans[0][0])*scale\n",
    "for i in range(len(ch)):\n",
    "    ch[i][0] = (ch[i][0]-ch[i][0][0])*scale\n",
    "ch_nu = pl.arange(0,len(ch))\n",
    "\n",
    "f, ax = pl.subplots(2,1, figsize=(10,4))\n",
    "ax[0].plot(trans[0][::50], trans[1][::50])\n",
    "\n",
    "for c in ch:\n",
    "    ax[1].plot(c[0][::500],c[1][::500])\n",
    "#ax[1].set_xlim(5,5+.1)\n",
    "iplot_mpl(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the demodulation signal to enhance the peak. We use `ch_filter` function and a `th` threshold value. The figure shows a fragment of the demodulation signal, filtered and with the threshold.\n",
    "\n",
    "It is typically inadvisable to tune the `dem_alpha` (the filter corner) but tuning of `dem_th` is required. You want it sufficiently above the noise floor to reject any false comb teeth, but low enough to still see each tooth. \n",
    "\n",
    "To check sucess, see how many peaks the program had to manually insert, the number should be approximately less than 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dem_th =6e-7\n",
    "dem_alpha = 90\n",
    "\n",
    "## Demod signal\n",
    "def ch_filter(d, alpha = 100):\n",
    "    d5 = gaussian_filter1d(d,3)\n",
    "    d6 = laplace(d5)\n",
    "    return(d5-alpha*d6)\n",
    "\n",
    "avgsize=4\n",
    "f, ax = pl.subplots(len(ch),1, figsize=(20,8))\n",
    "for i, c in enumerate(ch):\n",
    "    #ax[i].plot(c[0],c[1],'k',lw=2)\n",
    "    ax[i].plot(c[0],ch_filter(c[1],dem_alpha), '-',lw=2,)\n",
    "    ax[i].set_xlim(10.2,10.23)\n",
    "    ax[i].set_ylim(-1e-8,+15e-7)\n",
    "    ax[i].axhline(dem_th,ls='--',c = 'k')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we analyze the pattern of peaks, and force pattern completion to account for any mis-fit peaks. The output graph, shows the pattern around one of the breaks in the expected pattern, and can be used to diagnosis whether the threshold is too low (noise is inadvertently identified as a peak) or to high (a peak is inadvertently ignored).\n",
    "\n",
    "`new points` is the number of breaks in the expected pattern, and you should tune `dem_th` until the value is less than about 5000 (but this is just an order of magnitude, dont worry if its a little more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_peaks(t, c, min_dist = 0.000000001):\n",
    "    ## isolate each peak\n",
    "    l, n = label(c)\n",
    "    #peaks = r_[[average(t[l==i]) for i in  arange(1,n)]].T\n",
    "    peaks = np.array([average(t[obj]) for obj in find_objects(l)])\n",
    "    size = np.array([len(l[obj])*(t[1]-t[0]) for obj in find_objects(l)])\n",
    "    peaks = peaks[:-1][diff(peaks)>min_dist]\n",
    "    return peaks, size\n",
    "\n",
    "\n",
    "peaks = [get_peaks(c[0],ch_filter(c[1],dem_alpha)>dem_th) for c in ch]\n",
    "\n",
    "# # deletes repeated peak lcoations, if coincidently present\n",
    "# repeated1=[]\n",
    "# repeated2=[]\n",
    "# for i in range(len(val)):\n",
    "#     repeated1.append(np.where(peaks[:][0][0]==val[i])[0][0])\n",
    "#     repeated2.append(np.where(peaks[:][1][0]==val[i])[0][0])\n",
    "\n",
    "\n",
    "peaks_c = hstack([ones(len(peaks[i][0])) * ch_nu[i] for i in range(len(peaks)) ]) # Channel\n",
    "peaks_t = hstack([peaks[i][0] for i in range(len(peaks)) ]) # T\n",
    "\n",
    "o_peaks_c = peaks_c[argsort(peaks_t)].astype(np.int16)\n",
    "o_peaks_t = sort(peaks_t)\n",
    "    \n",
    "val=np.where(np.diff(o_peaks_t)==0)[0]\n",
    "o_peaks_t=np.delete(o_peaks_t,val)\n",
    "o_peaks_c=np.delete(o_peaks_c,val)\n",
    "\n",
    "print('Found {:} peaks'.format(len(o_peaks_c)))\n",
    "\n",
    "patch_pattern = cycle([1,0,0,1])\n",
    "do_patch = True\n",
    "\n",
    "def patch(t,c, pattern):\n",
    "    new_p = []\n",
    "    rem_p = []\n",
    "    for i in range(len(c)-1):\n",
    "        step = 0\n",
    "        symbol = []\n",
    "        while True:\n",
    "            x = next(pattern)\n",
    "            if (i+1) >= len(c): break\n",
    "            if c[i+1] != x:  #look ahead\n",
    "                symbol.append(x)\n",
    "                if len(symbol)>4:  #it probably means we need to remove the peak\n",
    "                    rem_p.append(i)\n",
    "                    for j in range(3):\n",
    "                        next(pattern)\n",
    "                    break\n",
    "            else: ## add all the missing patterns\n",
    "                for j,x in enumerate(symbol):\n",
    "                    val=(t[i] + (t[i+1]-t[i])*(j+1)/(len(symbol)+1), x)\n",
    "                    new_p.append(val)\n",
    "                break\n",
    "    \n",
    "    #t = np.delete(t,rem_p)\n",
    "    #c = np.delete(c,rem_p)\n",
    "    \n",
    "    new_p = np.array(new_p).T\n",
    "    return new_p\n",
    "\n",
    "# We expect peaks beeing 12344321.. so let's make sure this sequence repeats\n",
    "t,c = o_peaks_t[:], o_peaks_c[:]\n",
    "f,axes = pl.subplots(3,1, figsize=(20,6), sharex=True)\n",
    "\n",
    "# Before patch\n",
    "axes[0].plot(t[1:],diff(c),'o-')\n",
    "axes[0].plot(t,c,'o-')\n",
    "print('Original number of points {:}'.format(len(t)))\n",
    "\n",
    "new_p = patch(t,c,patch_pattern)        \n",
    "if do_patch:\n",
    "    t = np.r_[t,new_p[0]]\n",
    "    c = np.r_[c,new_p[1]]\n",
    "    c = c[argsort(t)]\n",
    "    t = sort(t)\n",
    "\n",
    "#removes repeated indices\n",
    "#t=np.array(sorted(list(set(t))))\n",
    "\n",
    "#After patch\n",
    "axes[1].plot(t[1:],diff(c),'o-')\n",
    "axes[1].plot(t,c,'o-')\n",
    "print('New points {:}'.format(len(new_p[0])))\n",
    "\n",
    "axes[2].plot(ch[0][0],ch[0][1])\n",
    "axes[2].plot(ch[1][0],ch[1][1])\n",
    "\n",
    "#axes.set_ylim(0,4)\n",
    "t0 = new_p[0,100]\n",
    "axes[0].set_xlim(t0- 2e-2, t0+2e-2)\n",
    "axes[2].set_ylim(0,5e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, based on the corrected pattern of modulation peaks, we can extract frequency versus time. This is done by using the comb spacing as a ruler, counting the frequency we have sweeped in terms of the number of comb teeth we hve seen, then interpolating between teeth to fill out the rest of the spectrum.\n",
    "\n",
    "This gives us an estimate for the frequency we have swept versus the time of the sweep, which combined with the initial wavelength, allows frequency knowledge of the laser at the Mhz level for the entire sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Unwrap the channels\n",
    "mod = [25,100]       # Demod freq\n",
    "mod = [10,40]       # Demod freq\n",
    "rep_rate = 250 # Menlo reprate\n",
    "\n",
    "deri = diff(c)\n",
    "\n",
    "deri2 = np.r_[0,diff(deri)]\n",
    "n_p = (c[1:]+1)\n",
    "\n",
    "n_p[c[1:]==0] = mod[0]\n",
    "n_p[c[1:]==1] = mod[1]\n",
    "\n",
    "###############\n",
    "## First unwrap the two halves\n",
    "n_p = where(deri > 0, n_p, rep_rate-n_p) # The zero point\n",
    "n_p = where(np.logical_and(deri == 0, deri2>0), mod[0], n_p )\n",
    "\n",
    "## Every passage from zero, add rep rate\n",
    "n_p = n_p + np.cumsum(where(np.logical_and(deri == 0, deri2>0), 1, 0) )*rep_rate\n",
    "\n",
    "## Trendline\n",
    "freq_fit = np.poly1d(np.polyfit(t[1::100], n_p[::100],2))\n",
    "\n",
    "#find points that are clearly off and remove them\n",
    "#rem_p = where(diff(freq_f(t)-freq_fit(t))>205)[0]\n",
    "#print(rem_p[:10])\n",
    "n_t = t[1:]\n",
    "\n",
    "\n",
    "## T vs Frequency !!\n",
    "#freq_f = lambda x: np.interp(x, t[1:], n_p)\n",
    "#feq_f = InterpolatedUnivariateSpline(n_t, n_p)\n",
    "#Piece_wise spline\n",
    "block = 50\n",
    "np_spline = []\n",
    "i = 0\n",
    "while i< len(n_p):\n",
    "    sl = slice(i,min(i+block,len(n_p)))\n",
    "    np_spline.append(UnivariateSpline(n_t[sl], n_p[sl], k=5,s=50000)(n_t[sl]))\n",
    "    i = i+block\n",
    "    \n",
    "n_p2 = np.hstack(np_spline)\n",
    "freq_f = InterpolatedUnivariateSpline(n_t, n_p2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the frequency versus time of the scan, with a linear trend removed. This is the deviation from a perfectly linear laser sweep, in Mhz.\n",
    "\n",
    "Typical behavior depends on laser used, but fluctuation on a short time sclae (consitent with piezo motion) and slower drift (consitent with thermal drift) are common.\n",
    "\n",
    "adjust the \"cut\" value below, to remove any spurios data at the end of the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f,ax = pl.subplots(1,1, figsize=(8,4))\n",
    "cut=int(65e5)\n",
    "t0cut=trans[0,0:cut]\n",
    "t1cut=trans[1,0:cut]\n",
    "\n",
    "ax.plot(n_t,n_p2-freq_fit(n_t))\n",
    "ax.plot(t0cut[::100],freq_f(t0cut[::100])-freq_fit(t0cut[::100]))\n",
    "#ax.set_xlim(0,95)\n",
    "ax.set_ylim(-2000,1500)\n",
    "iplot_mpl(f)\n",
    "\n",
    "#use this to make sure laser looks ok, and cut the the end of the scan if the sweep ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the spectrum. This final result, is the obsevred transmission, versus frequency. Note, that the program has no way of telling which way you sweep, the units here are absolute frequency difference (in Mhz) from the start of the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Save transmission and frequency in one file\n",
    "savemat(filename+'_tr', {'data': np.vstack([freq_f(trans[0,0:cut]), trans[1,0:cut]],)}),\n",
    "                        #'options': dict(t_range = t_range,\n",
    "                        #                dem_th = dem_th,\n",
    "                        #                dem_alpha = dem_alpha,\n",
    "                        #                do_patch = do_patch,\n",
    "                        #                patch_pattern = patch_pattern)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cut=int(65e5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
